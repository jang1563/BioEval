{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioEval Results Analysis\n",
    "\n",
    "This notebook analyzes evaluation results from the BioEval benchmark suite.\n",
    "\n",
    "## Components\n",
    "1. **ProtoReason**: Protocol procedural reasoning\n",
    "2. **CausalBio**: Causal perturbation prediction\n",
    "3. **DesignCheck**: Experimental design critique\n",
    "4. **Error Taxonomy**: Failure mode analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "RESULTS_DIR = Path('../results')\n",
    "\n",
    "def load_results(filepath):\n",
    "    \"\"\"Load results from JSON file.\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# List available result files\n",
    "result_files = list(RESULTS_DIR.glob('*.json'))\n",
    "print(f\"Found {len(result_files)} result files:\")\n",
    "for f in result_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest results (modify path as needed)\n",
    "# results = load_results(RESULTS_DIR / 'claude-sonnet_20250108.json')\n",
    "\n",
    "# For demo, create sample results structure\n",
    "sample_results = {\n",
    "    \"metadata\": {\n",
    "        \"model\": \"claude-sonnet-4-20250514\",\n",
    "        \"timestamp\": \"2025-01-08T12:00:00\"\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_tasks\": 75,\n",
    "        \"by_component\": {\n",
    "            \"protoreason\": {\"num_tasks\": 30, \"completed\": 30},\n",
    "            \"causalbio\": {\"num_tasks\": 35, \"completed\": 35},\n",
    "            \"designcheck\": {\"num_tasks\": 10, \"completed\": 10}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Results loaded successfully\")\n",
    "print(f\"Model: {sample_results['metadata']['model']}\")\n",
    "print(f\"Total tasks: {sample_results['summary']['total_tasks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ProtoReason Analysis\n",
    "\n",
    "Analyze protocol procedural reasoning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ProtoReason results for visualization\n",
    "protoreason_data = {\n",
    "    'task_type': ['step_ordering', 'step_ordering', 'missing_step', 'missing_step', \n",
    "                  'calculation', 'calculation', 'calculation', 'troubleshooting', \n",
    "                  'troubleshooting', 'safety'],\n",
    "    'protocol': ['western_blot', 'qpcr', 'western_blot', 'chip_seq',\n",
    "                 'dilution', 'protein', 'cell_counting', 'western_blot',\n",
    "                 'qpcr', 'lentivirus'],\n",
    "    'score': [0.85, 0.92, 0.75, 0.60, 0.95, 0.88, 0.92, 0.80, 0.72, 0.90],\n",
    "    'response_length': [450, 380, 520, 610, 280, 350, 290, 680, 720, 420]\n",
    "}\n",
    "\n",
    "df_proto = pd.DataFrame(protoreason_data)\n",
    "df_proto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProtoReason: Performance by task type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart by task type\n",
    "task_scores = df_proto.groupby('task_type')['score'].mean().sort_values()\n",
    "ax1 = axes[0]\n",
    "task_scores.plot(kind='barh', ax=ax1, color=sns.color_palette('viridis', len(task_scores)))\n",
    "ax1.set_xlabel('Average Score')\n",
    "ax1.set_ylabel('Task Type')\n",
    "ax1.set_title('ProtoReason: Performance by Task Type')\n",
    "ax1.set_xlim(0, 1)\n",
    "for i, v in enumerate(task_scores):\n",
    "    ax1.text(v + 0.02, i, f'{v:.2f}', va='center')\n",
    "\n",
    "# Box plot by task type\n",
    "ax2 = axes[1]\n",
    "df_proto.boxplot(column='score', by='task_type', ax=ax2)\n",
    "ax2.set_xlabel('Task Type')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Score Distribution by Task Type')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('protoreason_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CausalBio Analysis\n",
    "\n",
    "Analyze causal perturbation prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample CausalBio results\n",
    "causalbio_data = {\n",
    "    'task_type': ['knockout', 'knockout', 'knockout', 'knockout', 'knockout',\n",
    "                  'pathway', 'pathway', 'pathway', 'pathway',\n",
    "                  'drug_response', 'drug_response', 'drug_response',\n",
    "                  'epistasis', 'epistasis', 'epistasis'],\n",
    "    'reasoning_type': ['oncogene_addiction', 'tumor_suppressor', 'synthetic_lethality', \n",
    "                       'core_essential', 'context_dependency',\n",
    "                       'RTK', 'MAPK', 'PI3K', 'cell_cycle',\n",
    "                       'kinase_inhibitor', 'epigenetic', 'targeted',\n",
    "                       'synthetic_lethal', 'suppressive', 'enhancing'],\n",
    "    'effect_correct': [True, True, True, True, False,\n",
    "                       True, True, False, True,\n",
    "                       True, False, True,\n",
    "                       True, False, True],\n",
    "    'mechanism_score': [0.85, 0.70, 0.90, 0.95, 0.45,\n",
    "                        0.88, 0.82, 0.55, 0.78,\n",
    "                        0.92, 0.60, 0.85,\n",
    "                        0.80, 0.50, 0.75],\n",
    "    'gene_or_drug': ['KRAS', 'TP53', 'PARP1', 'RPL13', 'KRAS-MCF7',\n",
    "                     'EGFR_inh', 'BRAF_inh', 'mTOR_inh', 'CDK4/6_inh',\n",
    "                     'Imatinib', 'JQ1', 'Dexamethasone',\n",
    "                     'BRCA1-PARP1', 'BRCA1-53BP1', 'KRAS-STK11']\n",
    "}\n",
    "\n",
    "df_causal = pd.DataFrame(causalbio_data)\n",
    "df_causal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CausalBio: Accuracy by task type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Effect prediction accuracy\n",
    "ax1 = axes[0]\n",
    "accuracy_by_type = df_causal.groupby('task_type')['effect_correct'].mean()\n",
    "colors = ['#2ecc71' if v > 0.7 else '#e74c3c' if v < 0.5 else '#f39c12' for v in accuracy_by_type]\n",
    "accuracy_by_type.plot(kind='bar', ax=ax1, color=colors)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Effect Prediction Accuracy')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Mechanism reasoning score\n",
    "ax2 = axes[1]\n",
    "df_causal.boxplot(column='mechanism_score', by='task_type', ax=ax2)\n",
    "ax2.set_ylabel('Mechanism Score')\n",
    "ax2.set_title('Mechanism Reasoning Quality')\n",
    "plt.suptitle('')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Scatter: effect correct vs mechanism score\n",
    "ax3 = axes[2]\n",
    "colors = df_causal['task_type'].map({'knockout': 'blue', 'pathway': 'green', \n",
    "                                      'drug_response': 'orange', 'epistasis': 'red'})\n",
    "ax3.scatter(df_causal['effect_correct'].astype(int) + np.random.normal(0, 0.05, len(df_causal)), \n",
    "            df_causal['mechanism_score'], c=colors, alpha=0.7, s=100)\n",
    "ax3.set_xlabel('Effect Correct (0=No, 1=Yes)')\n",
    "ax3.set_ylabel('Mechanism Score')\n",
    "ax3.set_title('Effect Accuracy vs Mechanism Quality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('causalbio_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CausalBio: Detailed knockout analysis by reasoning type\n",
    "df_ko = df_causal[df_causal['task_type'] == 'knockout'].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ko_scores = df_ko.groupby('reasoning_type').agg({\n",
    "    'effect_correct': 'mean',\n",
    "    'mechanism_score': 'mean'\n",
    "}).sort_values('mechanism_score')\n",
    "\n",
    "x = np.arange(len(ko_scores))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, ko_scores['effect_correct'], width, label='Effect Accuracy', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, ko_scores['mechanism_score'], width, label='Mechanism Score', color='#2ecc71')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Reasoning Type')\n",
    "ax.set_title('Knockout Prediction: Accuracy vs Reasoning Quality')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(ko_scores.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('knockout_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DesignCheck Analysis\n",
    "\n",
    "Analyze experimental design critique performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DesignCheck results\n",
    "designcheck_data = {\n",
    "    'design_id': [f'design_{i:03d}' for i in range(1, 11)],\n",
    "    'total_flaws': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "    'critical_flaws': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    'flaws_detected': [3, 2, 3, 2, 3, 2, 3, 2, 2, 3],\n",
    "    'critical_detected': [2, 1, 2, 2, 2, 1, 2, 1, 2, 2],\n",
    "    'false_positives': [0, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n",
    "    'design_type': ['drug_response', 'knockout', 'western_blot', 'gene_expression',\n",
    "                    'biomarker', 'comparison', 'scrnaseq', 'mechanism', 'screen', 'mouse']\n",
    "}\n",
    "\n",
    "df_design = pd.DataFrame(designcheck_data)\n",
    "df_design['flaw_recall'] = df_design['flaws_detected'] / df_design['total_flaws']\n",
    "df_design['critical_recall'] = df_design['critical_detected'] / df_design['critical_flaws']\n",
    "df_design['precision'] = df_design['flaws_detected'] / (df_design['flaws_detected'] + df_design['false_positives'])\n",
    "df_design.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DesignCheck: Flaw detection performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Overall metrics\n",
    "ax1 = axes[0]\n",
    "metrics = ['flaw_recall', 'critical_recall', 'precision']\n",
    "metric_values = [df_design[m].mean() for m in metrics]\n",
    "metric_labels = ['All Flaws\\nRecall', 'Critical Flaws\\nRecall', 'Precision']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = ax1.bar(metric_labels, metric_values, color=colors)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Overall Flaw Detection Performance')\n",
    "ax1.set_ylim(0, 1)\n",
    "for bar, val in zip(bars, metric_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# By design type\n",
    "ax2 = axes[1]\n",
    "df_design_sorted = df_design.sort_values('flaw_recall')\n",
    "ax2.barh(df_design_sorted['design_type'], df_design_sorted['flaw_recall'], color='#3498db')\n",
    "ax2.set_xlabel('Flaw Recall')\n",
    "ax2.set_title('Flaw Detection by Design Type')\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "# Recall vs Precision scatter\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(df_design['flaw_recall'], df_design['precision'], s=100, alpha=0.7)\n",
    "for i, row in df_design.iterrows():\n",
    "    ax3.annotate(row['design_type'], (row['flaw_recall'], row['precision']),\n",
    "                 fontsize=8, alpha=0.7)\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Recall vs Precision Trade-off')\n",
    "ax3.set_xlim(0, 1.1)\n",
    "ax3.set_ylim(0, 1.1)\n",
    "ax3.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('designcheck_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Taxonomy Analysis\n",
    "\n",
    "Analyze failure modes across all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample error annotations\n",
    "error_data = {\n",
    "    'error_category': ['knowledge', 'knowledge', 'reasoning', 'reasoning', 'reasoning',\n",
    "                       'procedural', 'procedural', 'uncertainty', 'uncertainty', 'communication'] * 3,\n",
    "    'error_type': ['factual_hallucination', 'outdated_info', 'causal_reversal', 'pathway_truncation',\n",
    "                   'overgeneralization', 'step_omission', 'reagent_confusion', 'overconfidence',\n",
    "                   'missing_uncertainty', 'jargon_misuse'] * 3,\n",
    "    'severity': ['critical', 'minor', 'major', 'major', 'minor',\n",
    "                 'critical', 'major', 'major', 'minor', 'minor'] * 3,\n",
    "    'component': ['causalbio'] * 10 + ['protoreason'] * 10 + ['designcheck'] * 10\n",
    "}\n",
    "\n",
    "df_errors = pd.DataFrame(error_data)\n",
    "print(f\"Total annotated errors: {len(df_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# By category\n",
    "ax1 = axes[0, 0]\n",
    "category_counts = df_errors['error_category'].value_counts()\n",
    "colors = sns.color_palette('Set2', len(category_counts))\n",
    "ax1.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "ax1.set_title('Errors by Category')\n",
    "\n",
    "# By severity\n",
    "ax2 = axes[0, 1]\n",
    "severity_colors = {'critical': '#e74c3c', 'major': '#f39c12', 'minor': '#3498db'}\n",
    "severity_counts = df_errors['severity'].value_counts()\n",
    "ax2.bar(severity_counts.index, severity_counts.values, \n",
    "        color=[severity_colors[s] for s in severity_counts.index])\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Errors by Severity')\n",
    "\n",
    "# Heatmap: category x component\n",
    "ax3 = axes[1, 0]\n",
    "pivot = df_errors.groupby(['error_category', 'component']).size().unstack(fill_value=0)\n",
    "sns.heatmap(pivot, annot=True, fmt='d', cmap='YlOrRd', ax=ax3)\n",
    "ax3.set_title('Error Distribution: Category Ã— Component')\n",
    "\n",
    "# Top error types\n",
    "ax4 = axes[1, 1]\n",
    "type_counts = df_errors['error_type'].value_counts().head(10)\n",
    "type_counts.plot(kind='barh', ax=ax4, color='#3498db')\n",
    "ax4.set_xlabel('Count')\n",
    "ax4.set_title('Top 10 Error Types')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_taxonomy_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary table\n",
    "summary_stats = {\n",
    "    'Component': ['ProtoReason', 'CausalBio', 'DesignCheck', 'Overall'],\n",
    "    'Tasks': [10, 15, 10, 35],\n",
    "    'Avg Score': [0.82, 0.75, 0.85, 0.80],\n",
    "    'Best Category': ['Calculations (0.92)', 'Knockout (0.85)', 'Drug Response (0.90)', '-'],\n",
    "    'Weakest Category': ['Missing Steps (0.68)', 'Epistasis (0.68)', 'Mechanism (0.72)', '-'],\n",
    "    'Critical Errors': [2, 5, 3, 10]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BIOEVAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_findings = \"\"\"\n",
    "## Key Findings\n",
    "\n",
    "### Strengths\n",
    "1. **Strong procedural knowledge**: Model performs well on calculation tasks and protocol step ordering\n",
    "2. **Good pattern recognition**: High accuracy on oncogene addiction predictions\n",
    "3. **Safety awareness**: Correctly identifies most safety requirements\n",
    "\n",
    "### Areas for Improvement\n",
    "1. **Causal reasoning**: Struggles with epistasis and complex genetic interactions\n",
    "2. **Missing step detection**: Often misses subtle but critical protocol steps\n",
    "3. **Overconfidence**: High confidence on incorrect pathway predictions\n",
    "\n",
    "### Systematic Error Patterns\n",
    "- **Pathway truncation**: Tends to describe early signaling but misses downstream effects\n",
    "- **Context blindness**: Fails to account for cell line-specific genetic background\n",
    "- **Temporal confusion**: Sometimes reverses cause and effect in biological processes\n",
    "\n",
    "### Recommendations\n",
    "1. Include more perturbation data in training for causal reasoning\n",
    "2. Improve uncertainty calibration - model should express doubt on complex interactions\n",
    "3. Better integration of genetic context (mutations, dependencies) in reasoning\n",
    "\"\"\"\n",
    "\n",
    "print(key_findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "df_summary.to_csv('bioeval_summary.csv', index=False)\n",
    "\n",
    "# Export detailed results\n",
    "df_proto.to_csv('protoreason_details.csv', index=False)\n",
    "df_causal.to_csv('causalbio_details.csv', index=False)\n",
    "df_design.to_csv('designcheck_details.csv', index=False)\n",
    "df_errors.to_csv('error_annotations.csv', index=False)\n",
    "\n",
    "print(\"Results exported to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Running Evaluation\n",
    "\n",
    "```bash\n",
    "# Run full evaluation\n",
    "python scripts/run_evaluation.py --model claude-sonnet-4-20250514 --component all\n",
    "\n",
    "# Run specific component\n",
    "python scripts/run_evaluation.py --model claude-sonnet-4-20250514 --component causalbio\n",
    "\n",
    "# Compare models\n",
    "python scripts/run_evaluation.py --model claude-sonnet-4-20250514 --output results/claude.json\n",
    "python scripts/run_evaluation.py --model gpt-4 --output results/gpt4.json\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
